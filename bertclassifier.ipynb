{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/saideepnigidala/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/saideepnigidala/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/saideepnigidala/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/sn/htccfbds6_96grjy1_fwvkfw0000gn/T/tmpm4ilw6qt\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "starting\n",
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saideepnigidala/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "import sys\n",
    "\n",
    "#parser = ArgumentParser()\n",
    "#parser.add_argument('-num_labels', action=\"store\", dest=\"num_labels\", type=int)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "#home = str(Path.home())\n",
    "\n",
    "train_path = '/Users/saideepnigidala/Downloads/project/LIAR-PLUS/dataset/train2.tsv'\n",
    "test_path = '/Users/saideepnigidala/Downloads/project/LIAR-PLUS/dataset/test2.tsv'\n",
    "val_path = '/Users/saideepnigidala/Downloads/project/LIAR-PLUS/dataset/val2.tsv'\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "val_df = pd.read_csv(val_path, sep=\"\\t\", header=None)\n",
    "\n",
    "# Fill nan (empty boxes) with 0\n",
    "train_df = train_df.fillna(0)\n",
    "test_df = test_df.fillna(0)\n",
    "val_df = val_df.fillna(0)\n",
    "\n",
    "train = train_df.values\n",
    "test = test_df.values\n",
    "val = val_df.values\n",
    "\n",
    "\n",
    "labels = {'train':[train[i][2] for i in range(len(train))], 'test':[test[i][2] for i in range(len(test))], 'val':[val[i][2] for i in range(len(val))]}\n",
    "statements = {'train':[train[i][3] for i in range(len(train))], 'test':[test[i][3] for i in range(len(test))], 'val':[val[i][3] for i in range(len(val))]}\n",
    "subjects = {'train':[train[i][4] for i in range(len(train))], 'test':[test[i][4] for i in range(len(test))], 'val':[val[i][4] for i in range(len(val))]}\n",
    "speakers = {'train':[train[i][5] for i in range(len(train))], 'test':[test[i][5] for i in range(len(test))], 'val':[val[i][5] for i in range(len(val))]}\n",
    "jobs = {'train':[train[i][6] for i in range(len(train))], 'test':[test[i][6] for i in range(len(test))], 'val':[val[i][6] for i in range(len(val))]}\n",
    "states = {'train':[train[i][7] for i in range(len(train))], 'test':[test[i][7] for i in range(len(test))], 'val':[val[i][7] for i in range(len(val))]}\n",
    "affiliations = {'train':[train[i][8] for i in range(len(train))], 'test':[test[i][8] for i in range(len(test))], 'val':[val[i][8] for i in range(len(val))]}\n",
    "credits = {'train':[train[i][9:14] for i in range(len(train))], 'test':[test[i][9:14] for i in range(len(test))], 'val':[val[i][9:14] for i in range(len(val))]}\n",
    "contexts = {'train':[train[i][14] for i in range(len(train))], 'test':[test[i][14] for i in range(len(test))], 'val':[val[i][14] for i in range(len(val))]}\n",
    "justification = {'train':[train[i][15] for i in range(len(train))], 'test':[test[i][15] for i in range(len(test))], 'val':[val[i][15] for i in range(len(val))]}\n",
    "\n",
    "if num_labels == 6:\n",
    "\n",
    "    def to_onehot(a):\n",
    "        a_cat = [0]*len(a)\n",
    "        for i in range(len(a)):\n",
    "            if a[i]=='true':\n",
    "                a_cat[i] = [1,0,0,0,0,0]\n",
    "            elif a[i]=='mostly-true':\n",
    "                a_cat[i] = [0,1,0,0,0,0]\n",
    "            elif a[i]=='half-true':\n",
    "                a_cat[i] = [0,0,1,0,0,0]\n",
    "            elif a[i]=='barely-true':\n",
    "                a_cat[i] = [0,0,0,1,0,0]\n",
    "            elif a[i]=='false':\n",
    "                a_cat[i] = [0,0,0,0,1,0]\n",
    "            elif a[i]=='pants-fire':\n",
    "                a_cat[i] = [0,0,0,0,0,1]\n",
    "            else:\n",
    "                print('Incorrect label')\n",
    "        return a_cat\n",
    "\n",
    "elif num_labels == 2:\n",
    "\n",
    "    def to_onehot(a):\n",
    "        a_cat = [0]*len(a)\n",
    "        for i in range(len(a)):\n",
    "            if a[i]=='true':\n",
    "                a_cat[i] = [1,0]\n",
    "            elif a[i]=='mostly-true':\n",
    "                a_cat[i] = [1,0]\n",
    "            elif a[i]=='half-true':\n",
    "                a_cat[i] = [1,0]\n",
    "            elif a[i]=='barely-true':\n",
    "                a_cat[i] = [0,1]\n",
    "            elif a[i]=='false':\n",
    "                a_cat[i] = [0,1]\n",
    "            elif a[i]=='pants-fire':\n",
    "                a_cat[i] = [0,1]\n",
    "            else:\n",
    "                print('Incorrect label')\n",
    "        return a_cat\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Invalid number of labels. The number of labels should be either 2 or 6')\n",
    "\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "labels_onehot = {'train':to_onehot(labels['train']), 'test':to_onehot(labels['test']), 'val':to_onehot(labels['val'])}\n",
    "\n",
    "\n",
    "# Preparing meta data\n",
    "\n",
    "#credit['train'][2]\n",
    "\n",
    "metadata = {'train':[0]*len(train), 'val':[0]*len(val), 'test':[0]*len(test)}\n",
    "\n",
    "for i in range(len(train)):\n",
    "    subject = subjects['train'][i]\n",
    "    if subject == 0:\n",
    "        subject = 'None'\n",
    "\n",
    "    speaker = speakers['train'][i]\n",
    "    if speaker == 0:\n",
    "        speaker = 'None'\n",
    "\n",
    "    job = jobs['train'][i]\n",
    "    if job == 0:\n",
    "        job = 'None'\n",
    "\n",
    "    state = states['train'][i]\n",
    "    if state == 0:\n",
    "        state = 'None'\n",
    "\n",
    "    affiliation = affiliations['train'][i]\n",
    "    if affiliation == 0:\n",
    "        affiliation = 'None'\n",
    "\n",
    "    context = contexts['train'][i]\n",
    "    if context == 0 :\n",
    "        context = 'None'\n",
    "\n",
    "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
    "\n",
    "    metadata['train'][i] = meta\n",
    "\n",
    "for i in range(len(val)):\n",
    "    subject = subjects['val'][i]\n",
    "    if subject == 0:\n",
    "        subject = 'None'\n",
    "\n",
    "    speaker = speakers['val'][i]\n",
    "    if speaker == 0:\n",
    "        speaker = 'None'\n",
    "\n",
    "    job = jobs['val'][i]\n",
    "    if job == 0:\n",
    "        job = 'None'\n",
    "\n",
    "    state = states['val'][i]\n",
    "    if state == 0:\n",
    "        state = 'None'\n",
    "\n",
    "    affiliation = affiliations['val'][i]\n",
    "    if affiliation == 0:\n",
    "        affiliation = 'None'\n",
    "\n",
    "    context = contexts['val'][i]\n",
    "    if context == 0 :\n",
    "        context = 'None'\n",
    "\n",
    "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
    "\n",
    "    metadata['val'][i] = meta\n",
    "\n",
    "for i in range(len(test)):\n",
    "    subject = subjects['test'][i]\n",
    "    if subject == 0:\n",
    "        subject = 'None'\n",
    "\n",
    "    speaker = speakers['test'][i]\n",
    "    if speaker == 0:\n",
    "        speaker = 'None'\n",
    "\n",
    "    job = jobs['test'][i]\n",
    "    if job == 0:\n",
    "        job = 'None'\n",
    "\n",
    "    state = states['test'][i]\n",
    "    if state == 0:\n",
    "        state = 'None'\n",
    "\n",
    "    affiliation = affiliations['test'][i]\n",
    "    if affiliation == 0:\n",
    "        affiliation = 'None'\n",
    "\n",
    "    context = contexts['test'][i]\n",
    "    if context == 0 :\n",
    "        context = 'None'\n",
    "\n",
    "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
    "\n",
    "    metadata['test'][i] = meta\n",
    "\n",
    "\n",
    "# Credit score calculation\n",
    "credit_score = {'train':[0]*len(train), 'val':[0]*len(val), 'test':[0]*len(test)}\n",
    "for i in range(len(train)):\n",
    "    credit = credits['train'][i]\n",
    "    if sum(credit) == 0:\n",
    "        score = 0.5\n",
    "    else:\n",
    "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
    "    credit_score['train'][i] = [score for i in range(2304)]\n",
    "\n",
    "for i in range(len(val)):\n",
    "    credit = credits['val'][i]\n",
    "    if sum(credit) == 0:\n",
    "        score = 0.5\n",
    "    else:\n",
    "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
    "    credit_score['val'][i] = [score for i in range(2304)]\n",
    "\n",
    "for i in range(len(test)):\n",
    "    credit = credits['test'][i]\n",
    "    if sum(credit) == 0:\n",
    "        score = 0.5\n",
    "    else:\n",
    "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
    "    credit_score['test'][i] = [score for i in range(2304)]\n",
    "\n",
    "\n",
    "class BertLayerNorm(nn.Module):\n",
    "        def __init__(self, hidden_size, eps=1e-12):\n",
    "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "            \"\"\"\n",
    "            super(BertLayerNorm, self).__init__()\n",
    "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "            self.variance_epsilon = eps\n",
    "\n",
    "        def forward(self, x):\n",
    "            u = x.mean(-1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "            return self.weight * x + self.bias\n",
    "\n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "    num_labels = 2\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels=2): # Change number of labels here.\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size*3, num_labels)\n",
    "        #self.fc1 = nn.Linear(config.hidden_size*2, 512)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "\n",
    "    '''def forward_once(self, x):\n",
    "        # Forward pass\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output'''\n",
    "\n",
    "    def forward_once(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        #logits = self.classifier(pooled_output)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self, input_ids1, input_ids2, input_ids3, credit_sc):\n",
    "        # forward pass of input 1\n",
    "        output1 = self.forward_once(input_ids1, token_type_ids=None, attention_mask=None, labels=None)\n",
    "        # forward pass of input 2\n",
    "        output2 = self.forward_once(input_ids2, token_type_ids=None, attention_mask=None, labels=None)\n",
    "\n",
    "        output3 = self.forward_once(input_ids3, token_type_ids=None, attention_mask=None, labels=None)\n",
    "\n",
    "        out = torch.cat((output1, output2, output3), 1)\n",
    "        #print(out.shape)\n",
    "\n",
    "        # Multiply the credit score with the output after concatnation\n",
    "\n",
    "        out = torch.add(credit_sc, out)\n",
    "\n",
    "        #out = self.fc1(out)\n",
    "        logits = self.classifier(out)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification(num_labels)\n",
    "\n",
    "\n",
    "# Loading the statements\n",
    "X_train = statements['train']\n",
    "y_train = labels_onehot['train']\n",
    "\n",
    "X_val = statements['val']\n",
    "y_val = labels_onehot['val']\n",
    "\n",
    "X_train = X_train + X_val\n",
    "y_train = y_train + y_val\n",
    "\n",
    "\n",
    "X_test = statements['test']\n",
    "y_test = labels_onehot['test']\n",
    "\n",
    "# Loading the justification\n",
    "X_train_just = justification['train']\n",
    "\n",
    "X_val_just = justification['val']\n",
    "\n",
    "X_train_just = X_train_just + X_val_just\n",
    "\n",
    "X_test_just = statements['test']\n",
    "\n",
    "\n",
    "# Loading the meta data\n",
    "X_train_meta = metadata['train']\n",
    "X_val_meta = metadata['val']\n",
    "X_train_meta = X_train_meta + X_val_meta\n",
    "X_test_meta = metadata['test']\n",
    "\n",
    "# Loading Credit scores\n",
    "\n",
    "X_train_credit = credit_score['train']\n",
    "X_val_credit = credit_score['val']\n",
    "X_train_credit = X_train_credit+X_val_credit\n",
    "X_test_credit = credit_score['test']\n",
    "\n",
    "\n",
    "# Small data partitioned for debugging\n",
    "'''X_train = X_train[:100]\n",
    "y_train = y_train[:100]\n",
    "\n",
    "X_test = X_test[:100]\n",
    "y_test = y_test[:100]\n",
    "\n",
    "X_train_just = X_train_just[:100]\n",
    "X_test_just = X_test_just[:100]\n",
    "\n",
    "X_train_meta = X_train_meta[:100]\n",
    "X_test_meta = X_test_meta[:100]\n",
    "\n",
    "X_train_credit = X_train_credit[:100]\n",
    "X_test_credit = X_test_credit[:100]'''\n",
    "\n",
    "max_seq_length_stat = 64\n",
    "max_seq_length_just = 256\n",
    "max_seq_length_meta = 32\n",
    "\n",
    "class text_dataset(Dataset):\n",
    "    def __init__(self,x_y_list, transform=None):\n",
    "\n",
    "        self.x_y_list = x_y_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        # Tokenize statements\n",
    "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
    "\n",
    "        if len(tokenized_review) > max_seq_length_stat:\n",
    "            tokenized_review = tokenized_review[:max_seq_length_stat]\n",
    "\n",
    "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
    "\n",
    "        padding = [0] * (max_seq_length_stat - len(ids_review))\n",
    "\n",
    "        ids_review += padding\n",
    "\n",
    "        assert len(ids_review) == max_seq_length_stat\n",
    "\n",
    "        #print(ids_review)\n",
    "        ids_review = torch.tensor(ids_review)\n",
    "\n",
    "        fakeness = self.x_y_list[4][index] # color\n",
    "        list_of_labels = [torch.from_numpy(np.array(fakeness))]\n",
    "\n",
    "\n",
    "        # Tokenize justifications\n",
    "        #print(self.x_y_list[1][6833])\n",
    "        #print(index)\n",
    "\n",
    "        # Making sure that if there is no justification in a row(nan value converted to 0 using pandas), give it a justification called 'No justification' for training to be possible.\n",
    "        if self.x_y_list[1][index] == 0:\n",
    "            self.x_y_list[1][index] = 'No justification'\n",
    "\n",
    "        tokenized_review_just = tokenizer.tokenize(self.x_y_list[1][index])\n",
    "\n",
    "        if len(tokenized_review_just) > max_seq_length_just:\n",
    "            tokenized_review_just = tokenized_review_just[:max_seq_length_just]\n",
    "\n",
    "        ids_review_just  = tokenizer.convert_tokens_to_ids(tokenized_review_just)\n",
    "\n",
    "        padding = [0] * (max_seq_length_just - len(ids_review_just))\n",
    "\n",
    "        ids_review_just += padding\n",
    "\n",
    "        assert len(ids_review_just) == max_seq_length_just\n",
    "\n",
    "        #print(ids_review)\n",
    "        ids_review_just = torch.tensor(ids_review_just)\n",
    "\n",
    "        fakeness = self.x_y_list[4][index] # color\n",
    "        list_of_labels = [torch.from_numpy(np.array(fakeness))]\n",
    "\n",
    "        # Tokenize metadata\n",
    "\n",
    "        tokenized_review_meta = tokenizer.tokenize(self.x_y_list[2][index])\n",
    "\n",
    "        if len(tokenized_review_meta) > max_seq_length_meta:\n",
    "            tokenized_review_meta = tokenized_review_meta[:max_seq_length_meta]\n",
    "\n",
    "        ids_review_meta  = tokenizer.convert_tokens_to_ids(tokenized_review_meta)\n",
    "\n",
    "        padding = [0] * (max_seq_length_meta - len(ids_review_meta))\n",
    "\n",
    "        ids_review_meta += padding\n",
    "\n",
    "        assert len(ids_review_meta) == max_seq_length_meta\n",
    "\n",
    "        #print(ids_review)\n",
    "        ids_review_meta = torch.tensor(ids_review_meta)\n",
    "\n",
    "        fakeness = self.x_y_list[4][index] # color\n",
    "        list_of_labels = [torch.from_numpy(np.array(fakeness))]\n",
    "\n",
    "        credit_scr = self.x_y_list[3][index] # Credit score\n",
    "\n",
    "        #ones_768 = np.ones((768))\n",
    "        #credit_scr = credit_scr * ones_768\n",
    "        credit_scr = torch.tensor(credit_scr)\n",
    "\n",
    "        return [ids_review, ids_review_just, ids_review_meta, credit_scr], list_of_labels[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Train Statements and Justifications\n",
    "train_lists = [X_train, X_train_just, X_train_meta, X_train_credit, y_train]\n",
    "\n",
    "# Test Statements and Justifications\n",
    "test_lists = [X_test, X_test_just, X_train_meta, X_test_credit, y_test]\n",
    "\n",
    "# Preparing the data (Tokenize)\n",
    "training_dataset = text_dataset(x_y_list = train_lists)\n",
    "test_dataset = text_dataset(x_y_list = test_lists)\n",
    "\n",
    "\n",
    "# Prepare the training dictionaries\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "                   }\n",
    "dataset_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(test_lists[0])}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            fakeness_corrects = 0\n",
    "\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, fakeness in dataloaders_dict[phase]:\n",
    "\n",
    "                inputs1 = inputs[0] # News statement input\n",
    "                inputs2 = inputs[1] # Justification input\n",
    "                inputs3 = inputs[2] # Meta data input\n",
    "                inputs4 = inputs[3] # Credit scores input\n",
    "\n",
    "                inputs1 = inputs1.to(device)\n",
    "                inputs2 = inputs2.to(device)\n",
    "                inputs3 = inputs3.to(device)\n",
    "                inputs4 = inputs4.to(device)\n",
    "\n",
    "                fakeness = fakeness.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    #print(inputs)\n",
    "                    outputs = model(inputs1, inputs2, inputs3, inputs4)\n",
    "\n",
    "                    outputs = F.softmax(outputs,dim=1)\n",
    "\n",
    "                    loss = criterion(outputs, torch.max(fakeness.float(), 1)[1])\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs1.size(0)\n",
    "\n",
    "\n",
    "                fakeness_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(fakeness, 1)[1])\n",
    "\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "            fakeness_acc = fakeness_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} fakeness_acc: {:.4f}'.format(\n",
    "                phase, fakeness_acc))\n",
    "\n",
    "            # Saving training acc and loss for each epoch\n",
    "            fakeness_acc1 = fakeness_acc.data\n",
    "            fakeness_acc1 = fakeness_acc1.cpu()\n",
    "            fakeness_acc1 = fakeness_acc1.numpy()\n",
    "            train_acc.append(fakeness_acc1)\n",
    "\n",
    "            #epoch_loss1 = epoch_loss.data\n",
    "            #epoch_loss1 = epoch_loss1.cpu()\n",
    "            #epoch_loss1 = epoch_loss1.numpy()\n",
    "            train_loss.append(epoch_loss)\n",
    "\n",
    "            if phase == 'val' and fakeness_acc > best_acc:\n",
    "                print('Saving with accuracy of {}'.format(fakeness_acc),\n",
    "                      'improved over previous {}'.format(best_acc))\n",
    "                best_acc = fakeness_acc\n",
    "\n",
    "                # Saving val acc and loss for each epoch\n",
    "                fakeness_acc1 = fakeness_acc.data\n",
    "                fakeness_acc1 = fakeness_acc1.cpu()\n",
    "                fakeness_acc1 = fakeness_acc1.numpy()\n",
    "                val_acc.append(fakeness_acc1)\n",
    "\n",
    "                #epoch_loss1 = epoch_loss.data\n",
    "                #epoch_loss1 = epoch_loss1.cpu()\n",
    "                #epoch_loss1 = epoch_loss1.numpy()\n",
    "                val_loss.append(epoch_loss)\n",
    "\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_test_noFC1_triBERT_binary_focalloss.pth')\n",
    "\n",
    "        print('Time taken for epoch'+ str(epoch+1)+ ' is ' + str((time.time() - epoch_start)/60) + ' minutes')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(float(best_acc)))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_acc, val_acc, train_loss, val_loss\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "lrlast = .0001\n",
    "lrmain = .00001\n",
    "optim1 = optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "\n",
    "   ])\n",
    "\n",
    "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''import focal_loss\n",
    "loss_args = {\"alpha\": 0.5, \"gamma\": 2.0}\n",
    "criterion = focal_loss.FocalLoss(*loss_args)'''\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 3 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n",
    "\n",
    "\n",
    "model_ft1, train_acc, val_acc, train_loss, val_loss = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=20)\n",
    "\n",
    "# Accuracy plots\n",
    "\n",
    "print(val_acc)\n",
    "print(val_loss)\n",
    "\n",
    "#plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val'], loc='upper left')\n",
    "#plt.show()\n",
    "plt.savefig('accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "print('Saved Accuracy plot')\n",
    "\n",
    "# Loss plots\n",
    "\n",
    "#plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val'], loc='upper right')\n",
    "#plt.show()\n",
    "plt.savefig('loss.png')\n",
    "plt.close()\n",
    "\n",
    "print('Saved Loss plot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
